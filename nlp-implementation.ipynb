{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\n# Function to scrape text from a website\ndef scrape_text(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    # Modify this based on the structure of the webpage to extract relevant text\n    text = ' '.join([p.text for p in soup.find_all('p')])\n    return text\n\n# Define URLs of Arabic websites related to your topic\nurls = [\n    'https://alkhalilarabic.com/',\n    'https://guidetoarabic.net/ar'\n]\n\n# Define a function to calculate text score based on some criteria\ndef calculate_text_score(text):\n    return len(text) / 1000  # Just a simple scaling factor for demonstration\n\n# Create a dictionary to store text and corresponding scores\ndata = {}\ntexts =[]\nscores =[]\n\n# Scrape text from each URL and calculate the score\nfor url in urls:\n    text = scrape_text(url)\n    texts.append(text)\n    score = calculate_text_score(text)\n    scores.append(score)\n    data[url] = {'text': text, 'score': score}\n\n# Print the data\nfor url, info in data.items():\n    print(f\"URL: {url}\")\n    print(f\"Text:\\n{info['text']}\")\n    print(f\"Score: {info['score']}\\n\")\n    \nprint(texts)\nprint(scores)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T23:32:48.571507Z","iopub.execute_input":"2024-04-07T23:32:48.572059Z","iopub.status.idle":"2024-04-07T23:32:54.624080Z","shell.execute_reply.started":"2024-04-07T23:32:48.572022Z","shell.execute_reply":"2024-04-07T23:32:54.623073Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"URL: https://alkhalilarabic.com/\nText:\nالخليل لتعليم اللغة العربية اكتسب المهارات اللغوية وتعرف على الثقافة العربية لتفتح عالمك مع منصة الخليل. تعلم العربية في دورات عن بعد على أيدي خبراء انطلق في تعلم العربية وصقل مهاراتك اللغوية في القراءة والكتابة والمحادثة والاستماع من خلال الدورات الأساسية على منصة الخليل تعلم اللغة العربية من الخبراء في اللغة الناطقين بالعربية، وانغمس في ثقافتها، وتعرف على قيمها وحضارتها. تعلم اللغة العربية باحترافية وطوّر مهاراتك اللغوية عبر منصتنا التعليمية المتطورة وأنت في أي موقع من العالم. بإمكانك ممارسة وتطوير مهارة المحادثة لساعات إضافية مع أفضل المدرسين العرب، وفي الأوقات التي تناسبك. اكتسب مهارات لغوية جديدة ترتبط بمجال عملك أو دراستك أو اهتمامك اللغوي في المجالات الدبلوماسية أو الإعلامية أو الأكاديمية أو غيرها أحد مشاريع شركة مؤتلف (إبانة)\nScore: 0.743\n\nURL: https://guidetoarabic.net/ar\nText:\nاللغة العربية هي لغة القرآن الكريم، وهي وسيلة لفهم النصوص الشرعية، والاستنباط الصحيح من النصوص، وتعلم اللغة العربية واجب على المسلمين لفهم القرآن الكريم والسنة النبوية، ولا بد من دراسة العلوم المرتبطة باللغة العربية مثل: علم النحو والبلاغة، والقواعد والإعراب، وعلم البيان والأدب. هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها. مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها. نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم، والتواصل معها مباشرة، من خلال الخارطة الرقمية التي تظهر في أعلى القسم.    هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها.  مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها.     نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم،   دليل إلكترونيّ جامع لخدمات تعليم العربية للناطقين بغيرها، وما يتعلق بها من مبادرات ومشاريع وأفكار، يقدم للعاملين في هذا الحقل ما يعينهم على ترشيد أعمالهم، والوقوف على مواطن الاحتياج والنقص،    الدليل إلى العربية أحد مبادرات مركز أصول\nScore: 1.566\n\n['الخليل لتعليم اللغة العربية اكتسب المهارات اللغوية وتعرف على الثقافة العربية لتفتح عالمك مع منصة الخليل. تعلم العربية في دورات عن بعد على أيدي خبراء انطلق في تعلم العربية وصقل مهاراتك اللغوية في القراءة والكتابة والمحادثة والاستماع من خلال الدورات الأساسية على منصة الخليل تعلم اللغة العربية من الخبراء في اللغة الناطقين بالعربية، وانغمس في ثقافتها، وتعرف على قيمها وحضارتها. تعلم اللغة العربية باحترافية وطوّر مهاراتك اللغوية عبر منصتنا التعليمية المتطورة وأنت في أي موقع من العالم. بإمكانك ممارسة وتطوير مهارة المحادثة لساعات إضافية مع أفضل المدرسين العرب، وفي الأوقات التي تناسبك. اكتسب مهارات لغوية جديدة ترتبط بمجال عملك أو دراستك أو اهتمامك اللغوي في المجالات الدبلوماسية أو الإعلامية أو الأكاديمية أو غيرها أحد مشاريع شركة مؤتلف (إبانة)', 'اللغة العربية هي لغة القرآن الكريم، وهي وسيلة لفهم النصوص الشرعية، والاستنباط الصحيح من النصوص، وتعلم اللغة العربية واجب على المسلمين لفهم القرآن الكريم والسنة النبوية، ولا بد من دراسة العلوم المرتبطة باللغة العربية مثل: علم النحو والبلاغة، والقواعد والإعراب، وعلم البيان والأدب. هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها. مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها. نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم، والتواصل معها مباشرة، من خلال الخارطة الرقمية التي تظهر في أعلى القسم.    هنا نحدثك عن فضل العربية ومكانتها بين اللغات وما فيها من دلائل الإعجاز، وعن منزلتها من الإسلام وعن الأسباب التي تدعوك لتعلمها وتعليمها.  مواد متنوعة تبحث في كيفية تعلم اللغة العربية، والطُّرق المُثلَى لتعليمها، والمناهج المتَّبعة في هذا المجال، مع إضاءات على بعض المشكلات اللغوية وطرق معالجتها.     نوفِّر لك في هذا القسم قاعدة بياناتٍ كبيرةً تحوي بيانات وعناوين الجامعات والأكاديميات ومعاهد ومراكز تعليم اللغة العربية في العالم، لكي يتمكَّن الجميع من البحث عن أقرب المراكز التعليمية إليهم،   دليل إلكترونيّ جامع لخدمات تعليم العربية للناطقين بغيرها، وما يتعلق بها من مبادرات ومشاريع وأفكار، يقدم للعاملين في هذا الحقل ما يعينهم على ترشيد أعمالهم، والوقوف على مواطن الاحتياج والنقص،    الدليل إلى العربية أحد مبادرات مركز أصول']\n[0.743, 1.566]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Define your RNN-based model\nclass RNNModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNNModel, self).__init__()\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out, _ = self.rnn(x)\n        # Ensure that the output has the correct shape\n        if len(out.shape) == 3:\n            # Take the last output of the sequence\n            out = out[:, -1, :]\n        else:\n            # Reshape the output if it's 2-dimensional\n            out = out.squeeze(1)\n        out = self.fc(out)\n        return out\n\n\n# Define your dataset class\nclass ArabicTextDataset(Dataset):\n    def __init__(self, texts, scores):\n        self.texts = texts\n        self.scores = scores\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        return self.texts[idx], self.scores[idx]\n\n# Preprocessing functions\ndef tokenize(text):\n    return word_tokenize(text)\n\ndef remove_stopwords(tokens):\n    stop_words = set(stopwords.words('arabic'))\n    return [word for word in tokens if word.lower() not in stop_words]\n\n# Preprocessing pipeline\nprocessed_texts = []\nfor text in texts:\n    tokens = tokenize(text)\n    tokens = remove_stopwords(tokens)\n    processed_texts.append(tokens)\n\n# Convert text to numerical representation (dummy representation)\nword_to_idx = {word: idx for idx, word in enumerate(set(np.hstack(processed_texts)))}\nnumerical_texts = [[word_to_idx[word] for word in text] for text in processed_texts]\n\nprint(word_to_idx)\n\n# Pad sequences to ensure consistent length\npadded_sequences = pad_sequence([torch.tensor(text) for text in numerical_texts], batch_first=True, padding_value=0)\n# Ensure all sequences have the same length (pad to length 10)\npadded_sequences = nn.functional.pad(padded_sequences, (0, 184 - padded_sequences.size(1)))\n\n# Convert to PyTorch tensors\ninputs = padded_sequences.float()\ntargets = torch.tensor(scores, dtype=torch.float32)\n\n# Define input_size after padding\ninput_size = len(word_to_idx)\n\n# Split data into train and test sets\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n\n# Define hyperparameters\nhidden_size = 128\noutput_size = 1\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 1\n\n# Create DataLoader\ntrain_dataset = ArabicTextDataset(train_inputs, train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = RNNModel(input_size, hidden_size, output_size)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), targets)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluate the model\nwith torch.no_grad():\n    test_outputs = model(test_inputs)\n    # Reshape the target tensor to match the output tensor size\n    test_targets_resized = test_targets.view(-1, 1)\n    test_loss = criterion(test_outputs, test_targets_resized)\n    print(f'Test Loss: {test_loss.item():.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:33:03.439820Z","iopub.execute_input":"2024-04-07T23:33:03.440746Z","iopub.status.idle":"2024-04-07T23:33:03.538287Z","shell.execute_reply.started":"2024-04-07T23:33:03.440705Z","shell.execute_reply":"2024-04-07T23:33:03.536794Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"{'الخبراء': 0, 'وأفكار،': 1, 'الجامعات': 2, 'الدورات': 3, 'العالم،': 4, 'النبوية،': 5, 'وطرق': 6, 'اللغوي': 7, 'النصوص،': 8, 'الاحتياج': 9, 'للعاملين': 10, 'أفضل': 11, 'مؤتلف': 12, 'والاستماع': 13, 'التعليمية': 14, 'الإعجاز،': 15, 'الأسباب': 16, 'الدليل': 17, 'ممارسة': 18, 'وعلم': 19, 'دلائل': 20, 'أقرب': 21, 'يتعلق': 22, 'المجال،': 23, 'عالمك': 24, 'العرب،': 25, 'بد': 26, 'الأكاديمية': 27, 'اللغة': 28, 'عملك': 29, 'الإعلامية': 30, 'المشكلات': 31, 'ترشيد': 32, 'ومعاهد': 33, 'والطُّرق': 34, 'لتعلمها': 35, 'لساعات': 36, '(': 37, 'ومكانتها': 38, 'وعناوين': 39, 'القراءة': 40, 'لتفتح': 41, 'إبانة': 42, 'مهاراتك': 43, 'تظهر': 44, 'معها': 45, 'البيان': 46, 'العلوم': 47, 'دراستك': 48, 'والسنة': 49, 'ترتبط': 50, 'دراسة': 51, 'نوفِّر': 52, 'أصول': 53, 'المتَّبعة': 54, 'وانغمس': 55, 'لخدمات': 56, 'الجميع': 57, 'تدعوك': 58, 'إضاءات': 59, 'وتطوير': 60, 'كيفية': 61, 'الشرعية،': 62, 'أعمالهم،': 63, 'الرقمية': 64, 'دليل': 65, 'أيدي': 66, 'والوقوف': 67, 'إضافية': 68, 'القسم': 69, 'والمحادثة': 70, 'عبر': 71, 'معالجتها': 72, 'إليهم،': 73, 'لفهم': 74, 'اللغات': 75, 'مهارات': 76, 'المسلمين': 77, 'النحو': 78, 'تناسبك': 79, 'وأنت': 80, 'يتمكَّن': 81, 'باحترافية': 82, 'بمجال': 83, 'الخارطة': 84, 'المهارات': 85, 'المتطورة': 86, 'انطلق': 87, 'والأكاديميات': 88, 'ومراكز': 89, 'مبادرات': 90, 'منصة': 91, 'خلال': 92, 'المدرسين': 93, 'والاستنباط': 94, 'منصتنا': 95, 'اللغوية': 96, 'يعينهم': 97, 'تبحث': 98, 'غيرها': 99, 'ثقافتها،': 100, 'مواطن': 101, 'وتعليمها': 102, 'واجب': 103, 'والكتابة': 104, 'المحادثة': 105, 'تعليم': 106, 'الأساسية': 107, 'العربية،': 108, 'وعن': 109, 'العربية': 110, 'القرآن': 111, 'المراكز': 112, '.': 113, 'لغوية': 114, 'وتعرف': 115, 'الكريم': 116, 'خبراء': 117, 'الصحيح': 118, 'المرتبطة': 119, 'الأوقات': 120, 'وطوّر': 121, 'لغة': 122, 'والإعراب،': 123, 'بيانات': 124, 'مركز': 125, 'تعلم': 126, 'مشاريع': 127, 'الإسلام': 128, 'والمناهج': 129, 'أعلى': 130, 'باللغة': 131, ':': 132, 'إلكترونيّ': 133, 'متنوعة': 134, 'وحضارتها': 135, 'والتواصل': 136, 'بالعربية،': 137, 'الكريم،': 138, 'مباشرة،': 139, 'البحث': 140, 'المُثلَى': 141, 'جديدة': 142, 'شركة': 143, 'جامع': 144, 'فضل': 145, 'بغيرها،': 146, 'لتعليم': 147, 'بياناتٍ': 148, ')': 149, 'دورات': 150, 'الدبلوماسية': 151, 'مواد': 152, 'اهتمامك': 153, 'العالم': 154, 'قاعدة': 155, 'والبلاغة،': 156, 'للناطقين': 157, 'والأدب': 158, 'تحوي': 159, 'والنقص،': 160, 'المجالات': 161, 'ومشاريع': 162, 'الحقل': 163, 'نحدثك': 164, 'الخليل': 165, 'وصقل': 166, 'لتعليمها،': 167, 'قيمها': 168, 'وتعلم': 169, 'مهارة': 170, 'منزلتها': 171, 'وسيلة': 172, 'وهي': 173, 'النصوص': 174, 'بإمكانك': 175, 'موقع': 176, 'والقواعد': 177, 'يقدم': 178, 'الثقافة': 179, 'كبيرةً': 180, 'وفي': 181, 'اكتسب': 182, 'الناطقين': 183}\nEpoch [1/10], Loss: 0.2130\nEpoch [2/10], Loss: 1.2363\nEpoch [3/10], Loss: 0.1135\nEpoch [4/10], Loss: 0.0766\nEpoch [5/10], Loss: 0.0454\nEpoch [6/10], Loss: 0.0220\nEpoch [7/10], Loss: 0.0073\nEpoch [8/10], Loss: 0.0007\nEpoch [9/10], Loss: 0.0008\nEpoch [10/10], Loss: 0.0054\nTest Loss: 2.0124\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Define your Bidirectional RNN-based model with GRU\nclass BiGRUModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(BiGRUModel, self).__init__()\n        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # multiplied by 2 because of bidirectional GRU\n    \n    def forward(self, x):\n        out, _ = self.rnn(x)\n        # Ensure that the output has the correct shape\n        if len(out.shape) == 3:\n            # Take the last output of the sequence\n            out = out[:, -1, :]\n        else:\n            # Reshape the output if it's 2-dimensional\n            out = out.squeeze(1)\n        out = self.fc(out)\n        return out\n\n\n# Define your dataset class\nclass ArabicTextDataset(Dataset):\n    def __init__(self, texts, scores):\n        self.texts = texts\n        self.scores = scores\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        return self.texts[idx], self.scores[idx]\n\n# Preprocessing functions\ndef tokenize(text):\n    return word_tokenize(text)\n\ndef remove_stopwords(tokens):\n    stop_words = set(stopwords.words('arabic'))\n    return [word for word in tokens if word.lower() not in stop_words]\n\n# Preprocessing pipeline\nprocessed_texts = []\nfor text in texts:\n    tokens = tokenize(text)\n    tokens = remove_stopwords(tokens)\n    processed_texts.append(tokens)\n\n# Convert text to numerical representation (dummy representation)\nword_to_idx = {word: idx for idx, word in enumerate(set(np.hstack(processed_texts)))}\nnumerical_texts = [[word_to_idx[word] for word in text] for text in processed_texts]\n\n# Pad sequences to ensure consistent length\npadded_sequences = pad_sequence([torch.tensor(text) for text in numerical_texts], batch_first=True, padding_value=0)\n# Ensure all sequences have the same length (pad to length 10)\npadded_sequences = nn.functional.pad(padded_sequences, (0, 184 - padded_sequences.size(1)))\n\n# Convert to PyTorch tensors\ninputs = padded_sequences.float()\ntargets = torch.tensor(scores, dtype=torch.float32)\n\n# Define input_size after padding\ninput_size = len(word_to_idx)\n\n# Split data into train and test sets\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n\n# Define hyperparameters\nhidden_size = 128\noutput_size = 1\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 1\n\n# Create DataLoader\ntrain_dataset = ArabicTextDataset(train_inputs, train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = BiGRUModel(input_size, hidden_size, output_size)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), targets)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluate the model\nwith torch.no_grad():\n    test_outputs = model(test_inputs)\n    # Reshape the target tensor to match the output tensor size\n    test_targets_resized = test_targets.view(-1, 1)\n    test_loss = criterion(test_outputs, test_targets_resized)\n    print(f'Test Loss: {test_loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T22:16:16.630975Z","iopub.execute_input":"2024-04-07T22:16:16.632613Z","iopub.status.idle":"2024-04-07T22:16:16.725701Z","shell.execute_reply.started":"2024-04-07T22:16:16.632573Z","shell.execute_reply":"2024-04-07T22:16:16.724514Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 0.9155\nEpoch [2/10], Loss: 0.0563\nEpoch [3/10], Loss: 0.1458\nEpoch [4/10], Loss: 0.0191\nEpoch [5/10], Loss: 0.0189\nEpoch [6/10], Loss: 0.0204\nEpoch [7/10], Loss: 0.0191\nEpoch [8/10], Loss: 0.0159\nEpoch [9/10], Loss: 0.0116\nEpoch [10/10], Loss: 0.0073\nTest Loss: 1.6315\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Define your Bidirectional RNN-based model with LSTM\nclass BiLSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(BiLSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # multiplied by 2 because of bidirectional LSTM\n    \n    def forward(self, x):\n        out, _ = self.lstm(x)\n        # Ensure that the output has the correct shape\n        if len(out.shape) == 3:\n            # Take the last output of the sequence\n            out = out[:, -1, :]\n        else:\n            # Reshape the output if it's 2-dimensional\n            out = out.squeeze(1)\n        out = self.fc(out)\n        return out\n\n\n# Define your dataset class\nclass ArabicTextDataset(Dataset):\n    def __init__(self, texts, scores):\n        self.texts = texts\n        self.scores = scores\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        return self.texts[idx], self.scores[idx]\n\n# Preprocessing functions\ndef tokenize(text):\n    return word_tokenize(text)\n\ndef remove_stopwords(tokens):\n    stop_words = set(stopwords.words('arabic'))\n    return [word for word in tokens if word.lower() not in stop_words]\n\n# Preprocessing pipeline\nprocessed_texts = []\nfor text in texts:\n    tokens = tokenize(text)\n    tokens = remove_stopwords(tokens)\n    processed_texts.append(tokens)\n\n# Convert text to numerical representation (dummy representation)\nword_to_idx = {word: idx for idx, word in enumerate(set(np.hstack(processed_texts)))}\nnumerical_texts = [[word_to_idx[word] for word in text] for text in processed_texts]\n\n# Pad sequences to ensure consistent length\npadded_sequences = pad_sequence([torch.tensor(text) for text in numerical_texts], batch_first=True, padding_value=0)\n# Ensure all sequences have the same length (pad to length 10)\npadded_sequences = nn.functional.pad(padded_sequences, (0, 184 - padded_sequences.size(1)))\n\n# Convert to PyTorch tensors\ninputs = padded_sequences.float()\ntargets = torch.tensor(scores, dtype=torch.float32)\n\n# Define input_size after padding\ninput_size = len(word_to_idx)\n\n# Split data into train and test sets\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n\n# Define hyperparameters\nhidden_size = 128\noutput_size = 1\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 1\n\n# Create DataLoader\ntrain_dataset = ArabicTextDataset(train_inputs, train_targets)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize model, loss function, and optimizer\nmodel = BiLSTMModel(input_size, hidden_size, output_size)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for inputs, targets in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), targets)\n        loss.backward()\n        optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluate the model\nwith torch.no_grad():\n    test_outputs = model(test_inputs)\n    # Reshape the target tensor to match the output tensor size\n    test_targets_resized = test_targets.view(-1, 1)\n    test_loss = criterion(test_outputs, test_targets_resized)\n    print(f'Test Loss: {test_loss.item():.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T22:23:46.386103Z","iopub.execute_input":"2024-04-07T22:23:46.386657Z","iopub.status.idle":"2024-04-07T22:23:46.517661Z","shell.execute_reply.started":"2024-04-07T22:23:46.386621Z","shell.execute_reply":"2024-04-07T22:23:46.516223Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.6645\nEpoch [2/10], Loss: 0.2008\nEpoch [3/10], Loss: 0.0281\nEpoch [4/10], Loss: 0.0001\nEpoch [5/10], Loss: 0.0015\nEpoch [6/10], Loss: 0.0038\nEpoch [7/10], Loss: 0.0061\nEpoch [8/10], Loss: 0.0082\nEpoch [9/10], Loss: 0.0096\nEpoch [10/10], Loss: 0.0104\nTest Loss: 2.0562\n","output_type":"stream"}]}]}